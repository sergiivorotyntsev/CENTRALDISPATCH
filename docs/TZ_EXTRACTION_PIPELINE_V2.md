# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –ó–∞–¥–∞–Ω–∏–µ: –ü–∞–π–ø–ª–∞–π–Ω –ò–∑–≤–ª–µ—á–µ–Ω–∏—è –î–∞–Ω–Ω—ã—Ö –∏–∑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤
## –í–µ—Ä—Å–∏—è 2.0 | CENTRALDISPATCH

---

## –°–û–î–ï–†–ñ–ê–ù–ò–ï

1. [P0: –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è](#p0-–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ-—É—Ç–æ—á–Ω–µ–Ω–∏—è)
2. [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞-–¥–∞–Ω–Ω—ã—Ö)
3. [–ü–∞–π–ø–ª–∞–π–Ω –∏–∑–≤–ª–µ—á–µ–Ω–∏—è](#–ø–∞–π–ø–ª–∞–π–Ω-–∏–∑–≤–ª–µ—á–µ–Ω–∏—è)
4. [OCR —Å—Ç—Ä–∞—Ç–µ–≥–∏—è](#ocr-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
5. [Layout-Aware Extraction](#layout-aware-extraction)
6. [Auction Template Profiles](#auction-template-profiles)
7. [Observability](#observability)
8. [Phase 1: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞](#phase-1-–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
9. [Phase 2: Quick Fixes](#phase-2-quick-fixes)
10. [Phase 3: Layout-Aware Extraction](#phase-3-layout-aware-extraction)
11. [Golden Dataset –∏ –ú–µ—Ç—Ä–∏–∫–∏](#golden-dataset-–∏-–º–µ—Ç—Ä–∏–∫–∏)

---

## P0: –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –£–¢–û–ß–ù–ï–ù–ò–Ø

### P0-1: Canonical Keys ‚Äî –ï–¥–∏–Ω—ã–π –ù–∞–±–æ—Ä –ö–ª—é—á–µ–π

**–ü—Ä–∏–Ω—Ü–∏–ø:** –û–¥–∏–Ω –Ω–∞–±–æ—Ä –∫–ª—é—á–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ–∑–¥–µ: extraction ‚Üí UI ‚Üí blocking issues ‚Üí CD payload.

#### Canonical Keyset (listing_fields.py ‚Äî –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–∞–≤–¥—ã)

| –°–µ–∫—Ü–∏—è | –ö–ª—é—á | CD API Key | –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ |
|--------|------|------------|--------------|
| vehicle | `vehicle_vin` | `vehicles[0].vin` | ‚úì |
| vehicle | `vehicle_year` | `vehicles[0].year` | ‚úì |
| vehicle | `vehicle_make` | `vehicles[0].make` | ‚úì |
| vehicle | `vehicle_model` | `vehicles[0].model` | ‚úì |
| vehicle | `vehicle_color` | `vehicles[0].color` | |
| vehicle | `vehicle_type` | `vehicles[0].vehicleType` | ‚úì |
| vehicle | `vehicle_condition` | `vehicles[0].isOperable` | ‚úì |
| vehicle | `vehicle_lot` | `vehicles[0].lotNumber` | |
| pickup | `pickup_name` | `stops[0].locationName` | |
| pickup | `pickup_address` | `stops[0].address.street` | ‚úì |
| pickup | `pickup_city` | `stops[0].address.city` | ‚úì |
| pickup | `pickup_state` | `stops[0].address.state` | ‚úì |
| pickup | `pickup_zip` | `stops[0].address.postalCode` | ‚úì |
| pickup | `pickup_phone` | `stops[0].contact.phone` | |
| pickup | `pickup_contact` | `stops[0].contact.name` | |
| delivery | `delivery_name` | `stops[1].locationName` | |
| delivery | `delivery_address` | `stops[1].address.street` | ‚úì |
| delivery | `delivery_city` | `stops[1].address.city` | ‚úì |
| delivery | `delivery_state` | `stops[1].address.state` | ‚úì |
| delivery | `delivery_zip` | `stops[1].address.postalCode` | ‚úì |
| delivery | `delivery_phone` | `stops[1].contact.phone` | |
| additional | `available_date` | `availableDate` | ‚úì |
| additional | `expiration_date` | `expirationDate` | |
| additional | `trailer_type` | `trailerType` | ‚úì |
| additional | `external_id` | `externalId` | |
| additional | `buyer_id` | ‚Äî (internal) | |
| additional | `buyer_name` | ‚Äî (internal) | |
| additional | `sale_date` | ‚Äî (internal) | |
| notes | `transport_special_instructions` | `transportationReleaseNotes` | |

#### Back-Compat Mapping (Legacy ‚Üí Canonical)

| Legacy Key | Canonical Key | –£–¥–∞–ª–∏—Ç—å –ø–æ—Å–ª–µ |
|------------|---------------|---------------|
| `vin` | `vehicle_vin` | 2026-03-01 |
| `year` | `vehicle_year` | 2026-03-01 |
| `make` | `vehicle_make` | 2026-03-01 |
| `model` | `vehicle_model` | 2026-03-01 |
| `lot_number` | `vehicle_lot` | 2026-03-01 |
| `pickup_street` | `pickup_address` | 2026-03-01 |
| `delivery_street` | `delivery_address` | 2026-03-01 |
| `reference_id` | `external_id` | 2026-03-01 |

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:** –í `api/routes/extractions.py` –¥–æ–±–∞–≤–∏—Ç—å –º–∞–ø–ø–∏–Ω–≥:

```python
LEGACY_KEY_MAP = {
    "vin": "vehicle_vin",
    "year": "vehicle_year",
    # ... etc
}

def normalize_keys(outputs: dict) -> dict:
    """Convert legacy keys to canonical keys."""
    normalized = {}
    for key, value in outputs.items():
        canonical = LEGACY_KEY_MAP.get(key, key)
        normalized[canonical] = value
    return normalized
```

---

### P0-2: –ü–æ—Ä—è–¥–æ–∫ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ (Precedence) –¥–ª—è –ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è

**–ü—Ä–∞–≤–∏–ª–æ:** –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª–µ–π –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –≤ —Å–ª–µ–¥—É—é—â–µ–º –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞:

```
1. USER_OVERRIDE      ‚Äî —Ä—É—á–Ω–∞—è –ø—Ä–∞–≤–∫–∞ –≤ UI (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
2. WAREHOUSE_CONST    ‚Äî –∏–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ —Å–∫–ª–∞–¥–∞
3. AUCTION_CONST      ‚Äî –∏–∑ –ø—Ä–æ—Ñ–∏–ª—è –∞—É–∫—Ü–∏–æ–Ω–∞
4. EXTRACTED          ‚Äî –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
5. DEFAULT            ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
6. EMPTY              ‚Äî –ø—É—Å—Ç–æ (–Ω–∏–∑—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
```

#### –ö–∞–∫–∏–µ –ø–æ–ª—è –º–æ–∂–Ω–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –ø—Ä–∏ –≤—ã–±–æ—Ä–µ Warehouse:

| –ü–æ–ª–µ | Warehouse Override | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|------|-------------------|-------------|
| `delivery_name` | ‚úì | –í—Å–µ–≥–¥–∞ –∏–∑ warehouse |
| `delivery_address` | ‚úì | –í—Å–µ–≥–¥–∞ –∏–∑ warehouse |
| `delivery_city` | ‚úì | –í—Å–µ–≥–¥–∞ –∏–∑ warehouse |
| `delivery_state` | ‚úì | –í—Å–µ–≥–¥–∞ –∏–∑ warehouse |
| `delivery_zip` | ‚úì | –í—Å–µ–≥–¥–∞ –∏–∑ warehouse |
| `delivery_phone` | ‚úì | –í—Å–µ–≥–¥–∞ –∏–∑ warehouse |
| `transport_special_instructions` | ‚úì | Merge —Å –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–º |
| `vehicle_vin` | ‚úó | –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å |
| `pickup_*` | ‚úó | –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å |

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è:

```python
@dataclass
class FieldValue:
    value: Any
    source: ValueSource  # USER_OVERRIDE | WAREHOUSE_CONST | AUCTION_CONST | EXTRACTED | DEFAULT
    confidence: float    # 0.0-1.0 (–¥–ª—è EXTRACTED)
    evidence_block_id: Optional[int]  # –°—Å—ã–ª–∫–∞ –Ω–∞ –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞
    updated_at: datetime
```

---

### P0-3: Layout-Aware Extraction ‚Äî –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ê–ª–≥–æ—Ä–∏—Ç–º

#### –í—Ö–æ–¥:
- PDF —Ñ–∞–π–ª
- pdfplumber words: `[{text, x0, top, x1, bottom, page}, ...]`

#### –ê–ª–≥–æ—Ä–∏—Ç–º:

```python
def extract_blocks(pdf_path: str, y_tolerance: float = 3.0) -> List[TextBlock]:
    """
    –ê–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤ ‚Üí —Å—Ç—Ä–æ–∫ ‚Üí –±–ª–æ–∫–æ–≤.

    –®–∞–≥ 1: Cluster words –≤ lines –ø–æ top —Å –¥–æ–ø—É—Å–∫–æ–º y_tolerance
    –®–∞–≥ 2: Sort —Å–ª–æ–≤–∞ –≤–Ω—É—Ç—Ä–∏ line –ø–æ x0 (left-to-right)
    –®–∞–≥ 3: Merge lines –≤ blocks –ø–æ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–º—É gap
    –®–∞–≥ 4: Multi-column detection –ø–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é x0
    """
    blocks = []

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            words = page.extract_words()
            if not words:
                continue

            # Step 1: Group words into lines by y-coordinate
            lines = cluster_words_to_lines(words, y_tolerance)

            # Step 2: Sort words within each line by x-coordinate
            for line in lines:
                line['words'].sort(key=lambda w: w['x0'])

            # Step 3: Detect columns
            columns = detect_columns(lines, page.width)

            # Step 4: Build blocks respecting column order
            page_blocks = build_blocks_from_columns(lines, columns, page_num)
            blocks.extend(page_blocks)

    return blocks


def cluster_words_to_lines(words: List[dict], y_tolerance: float) -> List[dict]:
    """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫–∏ –ø–æ Y-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ."""
    if not words:
        return []

    # Sort by top coordinate
    sorted_words = sorted(words, key=lambda w: (w['top'], w['x0']))

    lines = []
    current_line = {'top': sorted_words[0]['top'], 'words': [sorted_words[0]]}

    for word in sorted_words[1:]:
        # Same line if within y_tolerance
        if abs(word['top'] - current_line['top']) <= y_tolerance:
            current_line['words'].append(word)
        else:
            lines.append(current_line)
            current_line = {'top': word['top'], 'words': [word]}

    lines.append(current_line)
    return lines


def detect_columns(lines: List[dict], page_width: float) -> List[tuple]:
    """
    Detect column boundaries based on x0 distribution.

    Returns list of (x_start, x_end) tuples for each column.
    """
    # Collect all x0 values
    x_positions = []
    for line in lines:
        for word in line['words']:
            x_positions.append(word['x0'])

    if not x_positions:
        return [(0, page_width)]

    # Simple heuristic: if there's a gap > 20% of page width, it's a column break
    x_positions.sort()

    # Find significant gaps
    gaps = []
    for i in range(1, len(x_positions)):
        gap = x_positions[i] - x_positions[i-1]
        if gap > page_width * 0.15:  # 15% threshold
            gaps.append((x_positions[i-1], x_positions[i]))

    # Build columns from gaps
    if not gaps:
        return [(0, page_width)]

    columns = []
    prev_end = 0
    for gap_start, gap_end in gaps:
        columns.append((prev_end, gap_start))
        prev_end = gap_end
    columns.append((prev_end, page_width))

    return columns


def build_blocks_from_columns(
    lines: List[dict],
    columns: List[tuple],
    page_num: int
) -> List[TextBlock]:
    """Build text blocks respecting column boundaries."""
    blocks = []

    for col_idx, (col_start, col_end) in enumerate(columns):
        col_lines = []

        for line in lines:
            # Filter words that belong to this column
            col_words = [
                w for w in line['words']
                if col_start <= w['x0'] < col_end
            ]
            if col_words:
                col_lines.append({
                    'top': line['top'],
                    'words': col_words,
                    'text': ' '.join(w['text'] for w in col_words)
                })

        # Merge consecutive lines into blocks
        if col_lines:
            block_text = '\n'.join(l['text'] for l in col_lines)
            bbox = (
                min(w['x0'] for l in col_lines for w in l['words']),
                min(l['top'] for l in col_lines),
                max(w['x1'] for l in col_lines for w in l['words']),
                max(l['top'] for l in col_lines) + 10  # Approximate bottom
            )

            blocks.append(TextBlock(
                text=block_text,
                page=page_num,
                bbox=bbox,
                reading_order=col_idx,
                column_index=col_idx
            ))

    return blocks
```

#### –°–∏—Å—Ç–µ–º–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç:
- **Origin:** Top-left (0, 0)
- **Units:** Points (1/72 inch)
- **–î–ª—è UI –ø–æ–¥—Å–≤–µ—Ç–∫–∏:** –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

```python
def bbox_to_percent(bbox: tuple, page_width: float, page_height: float) -> dict:
    """Convert bbox to percentage coordinates for UI overlay."""
    x0, y0, x1, y1 = bbox
    return {
        'left': x0 / page_width * 100,
        'top': y0 / page_height * 100,
        'width': (x1 - x0) / page_width * 100,
        'height': (y1 - y0) / page_height * 100,
    }
```

---

### P0-4: OCR –°—Ç—Ä–∞—Ç–µ–≥–∏—è

#### –ü—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è OCR:

| –£—Å–ª–æ–≤–∏–µ | –î–µ–π—Å—Ç–≤–∏–µ |
|---------|----------|
| `words_count == 0` | OCR –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω |
| `raw_text_len < 100` | OCR –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω |
| `raw_text_len < 500` AND `pages > 1` | –í–æ–∑–º–æ–∂–µ–Ω hybrid |
| –°—Ç—Ä–∞–Ω–∏—Ü–∞ –±–µ–∑ —Å–ª–æ–≤ –≤ multi-page PDF | OCR —Ç–æ–ª—å–∫–æ –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (hybrid) |

#### –†–µ–∂–∏–º—ã:

```python
class TextExtractionMode(Enum):
    PDF_TEXT = "pdf"      # Native text layer
    OCR = "ocr"           # Full OCR
    HYBRID = "hybrid"     # Mix of PDF text + OCR for some pages
```

#### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å OCRmyPDF:

```python
import subprocess
import tempfile

def ensure_text_layer(pdf_path: str) -> tuple[str, TextExtractionMode]:
    """
    Ensure PDF has a text layer, using OCR if needed.

    Returns: (path_to_pdf_with_text, mode_used)
    """
    # Check if text layer exists
    with pdfplumber.open(pdf_path) as pdf:
        total_words = sum(len(page.extract_words() or []) for page in pdf.pages)
        total_chars = sum(len(page.extract_text() or '') for page in pdf.pages)

    if total_words > 20 and total_chars > 100:
        return pdf_path, TextExtractionMode.PDF_TEXT

    # Need OCR - use ocrmypdf
    output_path = tempfile.mktemp(suffix='.pdf')

    try:
        result = subprocess.run([
            'ocrmypdf',
            '--skip-text',      # Don't re-OCR pages with text
            '--deskew',         # Fix skewed scans
            '--rotate-pages',   # Auto-rotate
            '--language', 'eng',
            '--output-type', 'pdf',
            pdf_path,
            output_path
        ], capture_output=True, timeout=120)

        if result.returncode == 0:
            # Determine mode based on whether we had some text
            mode = TextExtractionMode.HYBRID if total_words > 0 else TextExtractionMode.OCR
            return output_path, mode
        else:
            logger.warning(f"OCR failed: {result.stderr}")
            return pdf_path, TextExtractionMode.PDF_TEXT

    except subprocess.TimeoutExpired:
        logger.error("OCR timeout")
        return pdf_path, TextExtractionMode.PDF_TEXT
```

#### –ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤ extraction_runs:

```sql
ALTER TABLE extraction_runs ADD COLUMN text_mode TEXT;           -- 'pdf', 'ocr', 'hybrid'
ALTER TABLE extraction_runs ADD COLUMN words_count INTEGER;
ALTER TABLE extraction_runs ADD COLUMN raw_text_len INTEGER;
ALTER TABLE extraction_runs ADD COLUMN ocr_applied BOOLEAN;
ALTER TABLE extraction_runs ADD COLUMN ocr_confidence_avg REAL;
```

---

### P0-5: Auction Template Profiles ‚Äî JSON Schema

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "AuctionTemplateProfile",
  "type": "object",
  "required": ["auction_type", "version", "match", "fields"],
  "properties": {
    "auction_type": {
      "type": "string",
      "enum": ["COPART", "IAA", "MANHEIM", "OTHER"]
    },
    "version": {
      "type": "string",
      "pattern": "^\\d+\\.\\d+\\.\\d+$"
    },
    "match": {
      "type": "object",
      "description": "Rules for matching this profile to a document",
      "properties": {
        "sender_domains": {
          "type": "array",
          "items": {"type": "string"},
          "description": "Email sender domains (e.g., copart.com)"
        },
        "subject_keywords": {
          "type": "array",
          "items": {"type": "string"},
          "description": "Email subject keywords"
        },
        "text_indicators": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "pattern": {"type": "string"},
              "weight": {"type": "number", "default": 1.0}
            }
          }
        },
        "confidence_threshold": {
          "type": "number",
          "minimum": 0,
          "maximum": 1,
          "default": 0.6
        }
      }
    },
    "constants": {
      "type": "object",
      "description": "Fixed values for this auction type",
      "additionalProperties": {
        "type": "object",
        "properties": {
          "value": {},
          "description": {"type": "string"}
        }
      }
    },
    "fields": {
      "type": "object",
      "description": "Extraction rules per field",
      "additionalProperties": {
        "type": "object",
        "properties": {
          "extractor_type": {
            "type": "string",
            "enum": ["regex", "label_value", "table_cell", "spatial", "heuristic"]
          },
          "patterns": {
            "type": "array",
            "items": {"type": "string"},
            "description": "Regex patterns or label patterns"
          },
          "evidence_strategy": {
            "type": "string",
            "enum": ["nearest_block", "same_line", "below_label", "right_of_label"]
          },
          "postprocess": {
            "type": "array",
            "items": {
              "type": "string",
              "enum": ["normalize_state", "zip_fix", "vin_strip", "date_parse", "uppercase", "titlecase"]
            }
          },
          "confidence_rule": {
            "type": "object",
            "properties": {
              "type": {"type": "string", "enum": ["hard", "soft"]},
              "threshold": {"type": "number"}
            }
          },
          "fallback_value": {}
        }
      }
    },
    "guaranteed_fields": {
      "type": "array",
      "items": {"type": "string"},
      "description": "Fields this profile guarantees to extract (for quality metrics)"
    }
  }
}
```

#### –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è Copart:

```json
{
  "auction_type": "COPART",
  "version": "1.0.0",
  "match": {
    "sender_domains": ["copart.com", "copartmail.com"],
    "subject_keywords": ["Bill of Sale", "Sales Receipt", "Vehicle Release"],
    "text_indicators": [
      {"pattern": "SOLD THROUGH COPART", "weight": 5.0},
      {"pattern": "PHYSICAL ADDRESS OF LOT", "weight": 2.0},
      {"pattern": "copart.com", "weight": 4.0},
      {"pattern": "MEMBER:", "weight": 1.5}
    ],
    "confidence_threshold": 0.6
  },
  "constants": {
    "pickup_name_prefix": {
      "value": "Copart",
      "description": "Prefix for pickup location name"
    },
    "vehicle_condition_default": {
      "value": "OPERABLE",
      "description": "Default if not extracted"
    }
  },
  "fields": {
    "vehicle_vin": {
      "extractor_type": "regex",
      "patterns": ["VIN[:\\s]+([A-HJ-NPR-Z0-9]{17})", "\\b([A-HJ-NPR-Z0-9]{17})\\b"],
      "evidence_strategy": "nearest_block",
      "postprocess": ["vin_strip", "uppercase"],
      "confidence_rule": {"type": "hard", "threshold": 0.9}
    },
    "pickup_address": {
      "extractor_type": "label_value",
      "patterns": ["PHYSICAL ADDRESS OF LOT", "LOT LOCATION"],
      "evidence_strategy": "below_label",
      "postprocess": ["normalize_state"],
      "confidence_rule": {"type": "soft", "threshold": 0.7}
    },
    "buyer_id": {
      "extractor_type": "regex",
      "patterns": ["MEMBER[:\\s]+(\\d+)"],
      "evidence_strategy": "same_line",
      "confidence_rule": {"type": "hard", "threshold": 0.95}
    }
  },
  "guaranteed_fields": [
    "vehicle_vin",
    "vehicle_year",
    "vehicle_make",
    "vehicle_model",
    "buyer_id",
    "pickup_city",
    "pickup_state"
  ]
}
```

---

### P0-6: Golden Dataset –∏ –ú–µ—Ç—Ä–∏–∫–∏

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:

```
tests/golden_dataset/
‚îú‚îÄ‚îÄ copart/
‚îÇ   ‚îú‚îÄ‚îÄ doc_001.pdf
‚îÇ   ‚îú‚îÄ‚îÄ doc_001.json  # Ground truth
‚îÇ   ‚îú‚îÄ‚îÄ doc_002.pdf
‚îÇ   ‚îú‚îÄ‚îÄ doc_002.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ iaa/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ manheim/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ scans/
    ‚îî‚îÄ‚îÄ ...
```

#### –§–æ—Ä–º–∞—Ç ground truth JSON:

```json
{
  "document_id": "copart_001",
  "source": "copart",
  "is_scan": false,
  "fields": {
    "vehicle_vin": "1HGCV1F34LA123456",
    "vehicle_year": 2020,
    "vehicle_make": "Honda",
    "vehicle_model": "Accord",
    "pickup_address": "12020 US Highway 301 South",
    "pickup_city": "Riverview",
    "pickup_state": "FL",
    "pickup_zip": "33578"
  },
  "annotated_by": "human",
  "annotated_at": "2026-02-01T12:00:00Z"
}
```

#### –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø–æ–ª—è–º:

| –ü–æ–ª–µ | –ú–µ—Ç—Ä–∏–∫–∞ | –§–æ—Ä–º—É–ª–∞ | Target |
|------|---------|---------|--------|
| `vehicle_vin` | Exact Match | `extracted == ground_truth` | ‚â• 95% |
| `vehicle_year` | Exact Match | `int(extracted) == int(ground_truth)` | ‚â• 95% |
| `vehicle_make` | Fuzzy Match | `levenshtein(lower(extracted), lower(gt)) ‚â§ 2` | ‚â• 90% |
| `vehicle_model` | Fuzzy Match | `levenshtein(lower(extracted), lower(gt)) ‚â§ 3` | ‚â• 85% |
| `pickup_state` | Exact (normalized) | `normalize_state(extracted) == normalize_state(gt)` | ‚â• 95% |
| `pickup_zip` | Exact (5 digits) | `extracted[:5] == gt[:5]` | ‚â• 90% |
| `pickup_address` | Token Overlap | `jaccard(tokens(extracted), tokens(gt)) ‚â• 0.7` | ‚â• 80% |
| `pickup_city` | Fuzzy Match | `levenshtein ‚â§ 2` | ‚â• 85% |

#### Minimum Dataset Size:

| –¢–∏–ø | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç |
|-----|------------|-----------|
| Copart | 50 | P0 |
| IAA | 50 | P0 |
| Manheim | 30 | P1 |
| Scans (OCR) | 20 | P1 |
| **Total** | **150** | |

#### –¶–µ–ª–µ–≤—ã–µ –ø–æ—Ä–æ–≥–∏ –ø–æ —Ñ–∞–∑–∞–º:

| –ú–µ—Ç—Ä–∏–∫–∞ | Phase 2 | Phase 3 |
|---------|---------|---------|
| VIN Accuracy | ‚â• 90% | ‚â• 95% |
| Address Parts (city/state/zip) | ‚â• 75% | ‚â• 90% |
| Field Fill Rate | ‚â• 60% | ‚â• 80% |
| Zero-extraction rate | < 10% | < 2% |

---

### P0-7: Observability ‚Äî Must Log & Persist

#### –î–ª—è –∫–∞–∂–¥–æ–≥–æ extraction_run –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å:

```python
@dataclass
class ExtractionMetrics:
    # Ingestion
    ingest_ok: bool
    pdf_open_ok: bool
    pages_count: int
    file_size_bytes: int

    # Text extraction
    text_mode: str  # pdf, ocr, hybrid
    words_count: int
    raw_text_len: int
    ocr_applied: bool
    ocr_elapsed_ms: Optional[int]

    # Classification
    auction_detected: str
    detector_score: float
    detector_patterns_matched: List[str]

    # Field extraction
    fields_filled_count: int
    required_missing_count: int
    blocking_issues_count: int

    # Performance
    extraction_elapsed_ms: int

    # Errors
    errors: List[str]
    warnings: List[str]
```

#### API Endpoint: Run Debug (read-only)

```
GET /api/extractions/{id}/debug
```

Response:
```json
{
  "run_id": 123,
  "document_id": 456,
  "metrics": {
    "ingest_ok": true,
    "pdf_open_ok": true,
    "pages_count": 1,
    "text_mode": "pdf",
    "words_count": 178,
    "raw_text_len": 1202,
    "auction_detected": "IAA",
    "detector_score": 1.0,
    "fields_filled_count": 12,
    "required_missing_count": 2,
    "blocking_issues_count": 3
  },
  "raw_text_preview": "First 500 chars...",
  "detected_patterns": ["Insurance Auto Auctions", "Buyer Receipt"],
  "field_sources": {
    "vehicle_vin": {"value": "WA1CCAFP4GA133227", "source": "EXTRACTED", "confidence": 0.95},
    "delivery_address": {"value": "123 Main St", "source": "WAREHOUSE_CONST", "confidence": 1.0}
  }
}
```

#### UI: Preflight Diagnostics Banner

–í Review & Post –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–≤–µ—Ä—Ö—É:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üìä Extraction Details                                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Mode: PDF Text | Words: 178 | Chars: 1,202                  ‚îÇ
‚îÇ Detected: IAA (100%) | Fields: 12/14 | Issues: 3 blocking  ‚îÇ
‚îÇ ‚ö†Ô∏è Missing: pickup_address, pickup_phone                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

–ï—Å–ª–∏ `words_count == 0`:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ö†Ô∏è Document appears to be a scan without text layer         ‚îÇ
‚îÇ OCR processing required. [Run OCR] button                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## PHASE 1: –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê (Root Cause)

**–¶–µ–ª—å:** –¢–æ—á–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å, –ø–æ—á–µ–º—É –ø–æ—Å–ª–µ –∞–ø–ª–æ–∞–¥–∞ –ø–æ–ª—è –ø—É—Å—Ç—ã–µ.

### –ß–µ–∫-–ª–∏—Å—Ç

#### 1) –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
- [ ] –°–æ–±—Ä–∞—Ç—å 3‚Äì5 —Ä–µ–∞–ª—å–Ω—ã—Ö PDF (IAA/Copart + 1 —Å–∫–∞–Ω)
- [ ] –î–ª—è –∫–∞–∂–¥–æ–≥–æ: –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å doc_id/run_id, source, —Ç–∏–ø, —Ä–∞–∑–º–µ—Ä, —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≤ UI: Documents, Review & Post, Review & Train

#### 2) –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
–î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —à–∞–≥–∞–º:
- [ ] Ingestion: —Ñ–∞–π–ª –ø–æ–ª—É—á–µ–Ω, storage path, MIME, pages_count
- [ ] PDF Text Layer Check: –µ—Å—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç (true/false), char_count
- [ ] OCR (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω): –ø—Ä–∏–º–µ–Ω—è–ª—Å—è/–Ω–µ—Ç, –≤—Ä–µ–º—è, –∏—Ç–æ–≥–æ–≤—ã–π char_count
- [ ] Classification: detected_auction, score, features used
- [ ] Extraction: extractor_name, extracted_fields_count, empty_required_count
- [ ] Mapping ‚Üí Field Registry: —Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–π —Å–æ–≤–ø–∞–ª–æ, —Å–∫–æ–ª—å–∫–æ –ø–æ—Ç–µ—Ä—è–Ω–æ
- [ ] Persist: –∑–∞–ø–∏—Å—å –≤ –ë–î, —Ä–∞–∑–º–µ—Ä—ã, json keys
- [ ] API Response: —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–µ–∫ (counts, schema_version)

#### 3) –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ë–î
–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ run_id:
- [ ] `documents.raw_text`: NULL? –ø—É—Å—Ç–æ–π? –¥–ª–∏–Ω–∞?
- [ ] `extraction_runs.outputs_json`: NULL? –ø—É—Å—Ç–æ–π? keys?
- [ ] `extraction_runs.status`: –∫–∞–∫–∏–µ —Å—Ç–∞—Ç—É—Å—ã –≤—ã—Å—Ç–∞–≤–ª—è—é—Ç—Å—è?
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å: –Ω–µ –ø–µ—Ä–µ—Ç—ë—Ä–ª–∏ –ª–∏ outputs_json –ø—Ä–∏ Save Draft

#### 4) –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ "–ø—É—Å—Ç—ã–µ –ø–æ–ª—è" ‚Äî 4 –≥–∏–ø–æ—Ç–µ–∑—ã

**H1: –ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è / OCR –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è**
- [ ] –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å char_count=0 –¥–æ –∏ –ø–æ—Å–ª–µ OCR
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ OCR

**H2: –°–ª–æ–º–∞–ª–∞—Å—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**
- [ ] –°—Ä–∞–≤–Ω–∏—Ç—å score/auction_type —Å —ç—Ç–∞–ª–æ–Ω–æ–º
- [ ] –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

**H3: Mapping –ø–æ—Ç–µ—Ä—è –∫–ª—é—á–µ–π**
- [ ] –°—Ä–∞–≤–Ω–∏—Ç—å –∫–ª—é—á–∏ outputs_json vs registry keys
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å naming conventions

**H4: UI –Ω–µ –±–∏–Ω–¥–∏—Ç—Å—è –∫ API**
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å response –≤ Network tab
- [ ] –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ ID (run_id vs document_id)

#### 5) –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã Phase 1
- [ ] RC Report (1‚Äì2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã): –ø—Ä–∏—á–∏–Ω–∞ + –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
- [ ] Fix Plan: –∫–∞–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–¥—É—Ç –≤ Phase 2/3
- [ ] Smoke-tests: –º–∏–Ω–∏–º—É–º 3 —Ç–µ—Å—Ç–∞

#### 6) Exit Criteria
- [ ] –î–ª—è 3 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑–≤–µ—Å—Ç–Ω–æ: –Ω–∞ –∫–∞–∫–æ–º —ç—Ç–∞–ø–µ —Ç–µ—Ä—è—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ
- [ ] –í –ª–æ–≥–∞—Ö –µ—Å—Ç—å –ø–æ–ª–Ω—ã–π —Ç—Ä–µ–π—Å –ø–æ –æ–¥–Ω–æ–º—É run_id
- [ ] –ù–µ –æ—Å—Ç–∞–ª–æ—Å—å "unknown" –ø–æ: OCR, classification, mapping, UI-binding

---

## PHASE 2: QUICK FIXES

**–¶–µ–ª—å:** –†–µ–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞—á–∏–Ω–∞—é—Ç –∑–∞–ø–æ–ª–Ω—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è.

### –ß–µ–∫-–ª–∏—Å—Ç

#### 1) Text Layer / OCR fallback
- [ ] –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ char_count < threshold ‚Üí OCR pipeline
- [ ] –°–æ—Ö—Ä–∞–Ω—è—Ç—å: raw_text_before_ocr, raw_text_after_ocr, ocr_applied flag

#### 2) –ü–æ—á–∏–Ω–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
- [ ] –Ø–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã IAA/Copart: —Å–ª–æ–≤–∞—Ä—å/regex + confidence scoring
- [ ] –õ–æ–≥–∏: top-5 matched markers, –∏—Ç–æ–≥–æ–≤—ã–π score
- [ ] Fallback: –µ—Å–ª–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ ‚Üí generic extractor + "unknown_auction" tag

#### 3) Auction Template Profiles
- [ ] –°—É—â–Ω–æ—Å—Ç—å AuctionProfile: auction_type, fields_extracted[], constants[], parsing_rules
- [ ] UI –≤ Settings: –ø—Ä–æ—Å–º–æ—Ç—Ä –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è
- [ ] –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: profile_version, applied_to_run_id

#### 4) Warehouse Directory
- [ ] –í—ã–±–æ—Ä warehouse –∑–∞–ø–æ–ª–Ω—è–µ—Ç delivery stop fields
- [ ] –ó–∞–ø–æ–ª–Ω—è–µ—Ç transport_special_instructions
- [ ] –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å source –¥–ª—è delivery-–ø–æ–ª–µ–π

#### 5) Mapping "Extractor Output ‚Üí Field Registry"
- [ ] –ï–¥–∏–Ω—ã–π canonical keyset
- [ ] –°–ª–æ–π –∞–ª–∏–∞—Å–æ–≤ –¥–ª—è legacy keys
- [ ] Unit tests: –º–∏–Ω–∏–º—É–º 10 –∫–µ–π—Å–æ–≤

#### 6) Evidence/Provenance
- [ ] –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è: source_type, extractor_name, confidence, evidence_snippet

#### 7) UI: Review & Post / Review & Train
- [ ] –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å: –∑–Ω–∞—á–µ–Ω–∏—è, –∏—Å—Ç–æ—á–Ω–∏–∫, required/optional, –≤–∞–ª–∏–¥–∞—Ü–∏–∏
- [ ] Review & Train: –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –ø–æ–ª—è –∫–∞–∫ baseline

#### 8) Exit Criteria
- [ ] IAA/Copart: ‚â• 70% –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π –∞–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω—ã
- [ ] Warehouse –∑–∞–∫—Ä—ã–≤–∞–µ—Ç delivery + instructions
- [ ] –ù–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç "–≤—Å—ë –ø—É—Å—Ç–æ–µ" –Ω–∞ –≤–∞–ª–∏–¥–Ω—ã—Ö PDF
- [ ] –û—Ç—á—ë—Ç "Field Fill Rate" –ø–æ 20+ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º

---

## PHASE 3: LAYOUT-AWARE EXTRACTION

**–¶–µ–ª—å:** –£–±—Ä–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—É "–ª–∏–Ω–µ–π–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞", –ø–µ—Ä–µ–π—Ç–∏ –∫ block-based extraction.

### –ß–µ–∫-–ª–∏—Å—Ç

#### 1) –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ PDF
- [ ] `extract_document_structure()`: pages[], blocks[] —Å bbox
- [ ] Reading order sort (top-to-bottom, left-to-right)
- [ ] Multi-column detection

#### 2) –ù–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã
- [ ] `extracted_blocks`: run_id, page_num, block_index, bbox, text, reading_order
- [ ] `field_evidence`: run_id, field_key, evidence_type, page_num, bbox, snippet

#### 3) Block-aware extractors
- [ ] –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å extractors: –Ω–∞–π—Ç–∏ "—è–∫–æ—Ä–Ω—ã–µ" –±–ª–æ–∫–∏, –∑–∞—Ç–µ–º spatial window
- [ ] Address parser: street, city, state, zip + –≤–∞–ª–∏–¥–∞—Ç–æ—Ä

#### 4) Confusion cases
- [ ] 2+ –∫–æ–ª–æ–Ω–∫–∏
- [ ] –ê–¥—Ä–µ—Å —Ä–∞–∑–±–∏—Ç –ø–æ —Å—Ç—Ä–æ–∫–∞–º
- [ ] –ü–æ—Ö–æ–∂–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–æ–∫–∞—Ü–∏–π
- [ ] Fallback: regex –ø–æ normalized_text

#### 5) –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
- [ ] Field Fill Rate
- [ ] Address Accuracy (—Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞)
- [ ] Evidence Coverage
- [ ] Drift detection alarm

#### 6) Regression Dataset + —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] Dataset: 150 PDF minimum
- [ ] Golden labels: 25 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- [ ] –ê–≤—Ç–æ—Ç–µ—Å—Ç—ã: VIN/lot/addresses parsing

#### 7) UI enhancements
- [ ] Blocks overlay (debug mode)
- [ ] –ö–ª–∏–∫ –ø–æ –ø–æ–ª—é ‚Üí –ø–æ–¥—Å–≤–µ—Ç–∏—Ç—å evidence block

#### 8) Exit Criteria
- [ ] Pickup address ‚â• 90% –¥–ª—è IAA/Copart
- [ ] "–ü–µ—Ä–µ–ø—É—Ç–∞–Ω–Ω—ã–µ –∞–¥—Ä–µ—Å–∞" < 2%
- [ ] Evidence —É 80% auto-extracted –ø–æ–ª–µ–π
- [ ] –ù–µ—Ç –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø–æ Phase 2 –º–µ—Ç—Ä–∏–∫–∞–º

---

## –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø

### A. CD API V2 Validation Rules

| –ü–æ–ª–µ | –ü—Ä–∞–≤–∏–ª–æ |
|------|---------|
| `availableDate` | >= today, <= today+30 |
| `expirationDate` | >= today, <= today+30 |
| `desiredDeliveryDate` | >= availableDate, >= today, <= today+30 |
| `externalId` | <= 50 chars |
| `partnerReferenceId` | <= 50 chars |
| `stops` | exactly 2 |
| `vehicles` | 1-12 |
| `vehicles[].vin` | 17 chars, no I/O/Q |

### B. Rate Limiting

- Semaphore: max 3 concurrent CD API requests
- 429 ‚Üí respect Retry-After header
- 5xx ‚Üí max 3 attempts, exponential backoff (2s, 4s, 8s)

### C. ETag/If-Match

- GET returns ETag header
- PUT requires If-Match header with ETag
- 412 Precondition Failed ‚Üí re-fetch ETag and retry

---

**–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω:** 2026-02-02
**–í–µ—Ä—Å–∏—è:** 2.0
**–°—Ç–∞—Ç—É—Å:** Approved for Development
